{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534f90e1-c2a8-4782-a80d-3a212706ee6c",
   "metadata": {},
   "source": [
    "# Topic modeling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9fd0e1-0838-476b-8e08-a4a18736cb45",
   "metadata": {},
   "source": [
    "To perform a topic modeling analysis on a text and visualize the output, you can follow these steps:\n",
    "\n",
    "1. Preprocessing: Tokenize the text, remove stopwords and punctuation, and perform other necessary preprocessing steps.\n",
    "2. Dictionary and Corpus Creation: Create a dictionary and corpus required for LDA modeling.\n",
    "3. LDA Model Training: Train an LDA model on the corpus.\n",
    "4. Visualization: Use `pyLDAvis` to visualize the topics.\n",
    "\n",
    "The `num_topics` parameter in the `lda_analysis` function determines the number of topics to be extracted from the text. You can adjust this parameter based on your text content and the granularity of topics you wish to achieve.\n",
    "\n",
    "The `pyLDAvis` visualization will provide an interactive interface to explore the topics, their distribution, and the most relevant terms for each topic. You can hover over the topics and terms to get more detailed information.\n",
    "\n",
    "**Choosing the optimal number of topics for LDA** is more of an art than an exact science, as it depends on the specific characteristics of your dataset and your objectives. However, there are several approaches you can use to guide your decision:\n",
    "\n",
    "1. **Topic Coherence**: This is a popular method for evaluating the quality of the topics generated by LDA. Topic coherence measures the degree of semantic similarity between high-scoring words in the topic. You can use Gensim's `CoherenceModel` to calculate coherence scores for different numbers of topics and choose the number that maximizes coherence. You can then plot the coherence values against the number of topics to find the optimal number.\n",
    "2. **Perplexity**: This is another metric that can be used to evaluate the quality of the LDA model. Lower perplexity indicates a better model. However, perplexity might not always align with human judgment, so it's often used in conjunction with other methods.\n",
    "3. **Manual Inspection**: Sometimes, the best way to determine the optimal number of topics is to manually inspect the topics generated by the model for different numbers of topics. Look for a number that provides a meaningful and interpretable set of topics without too much overlap or redundancy.\n",
    "4. **Domain Knowledge**: Consider the context of your data and what you know about the subject area. If you have an idea of how many distinct topics you expect to find, this can guide your choice.\n",
    "\n",
    "It's often a good idea to experiment with several methods and consider their results in conjunction with your own judgment and the needs of your project.\n",
    "\n",
    "For advanced visualizations of LDA topic modeling results, you can explore various options beyond the standard pyLDAvis output. Here are some ideas:\n",
    "\n",
    "- **Topic Trend Analysis**: Track the prevalence of topics over time or across different segments of your data. This can be particularly useful if your corpus is time-stamped or can be divided into meaningful categories. Plotting the proportion of each topic in different time periods or segments can reveal trends and patterns in the data.\n",
    "- **Word Clouds for Topics**: Generate word clouds for each topic, where the size of each word is proportional to its importance in the topic. This provides a quick and visually appealing way to understand the content of each topic.\n",
    "- **Topic Network Visualization**: Create a network graph where nodes represent topics and edges represent the similarity between topics (based on their word distributions). This can help you visualize the relationships between topics and identify clusters of related topics.\n",
    "- **Topic Distribution in Documents**: For a selected set of documents, visualize the distribution of topics within each document using stacked bar charts or pie charts. This can help you understand how different topics are represented in individual documents or groups of documents.\n",
    "- **Interactive Dashboards**: Build interactive dashboards using tools like Dash or Streamlit that allow users to explore the LDA results dynamically. You could include options to filter documents by topic, search for specific words or topics, and visualize the results in various ways.\n",
    "- **Heatmaps of Topic-Word Distributions**: Create heatmaps to visualize the distribution of words across topics or the distribution of topics across documents. This can provide a detailed view of the relationships between words and topics or between documents and topics.\n",
    "- **Hierarchical Clustering of Topics**: Perform hierarchical clustering on the topics based on their word distributions and visualize the resulting dendrogram. This can help you identify groups of similar topics and understand the hierarchical structure of the topics in your corpus.\n",
    "\n",
    "## Anomaly Detection in textual data\n",
    "Leveraging Latent Dirichlet Allocation (LDA) for anomaly detection involves using the topic distributions generated by LDA to identify documents or text segments that are unusual or deviate significantly from the norm. Here are some approaches to using LDA for anomaly detection:\n",
    "\n",
    "- **Outlier Detection in Topic Distributions**: After training an LDA model on your corpus, you can examine the topic distribution for each document. Documents that have an unusual distribution of topics (e.g., a very high proportion of a single topic or a distribution that significantly differs from the average distribution) could be considered anomalies.\n",
    "\n",
    "- **Topic Coherence Analysis**: Calculate the coherence of topics generated by LDA. Topics with very low coherence (i.e., topics that contain a mix of unrelated words) might indicate anomalies in the data, such as documents that are very different from the rest of the corpus.\n",
    "\n",
    "- **Cluster Analysis of Topic Distributions**: Perform clustering (e.g., K-means or hierarchical clustering) on the topic distributions of documents. Documents that fall into very small clusters or that are far from the centroids of their clusters could be considered anomalies.\n",
    "\n",
    "- **Temporal Analysis of Topic Trends**: If your corpus is time-stamped, you can track the prevalence of topics over time. Sudden changes in topic prevalence or the emergence of new, transient topics could indicate anomalous events or shifts in the data.\n",
    "\n",
    "- **Comparison with a Reference Corpus**: If you have a reference corpus that represents \"normal\" data, you can train an LDA model on this corpus and then use it to analyze a target corpus. Documents in the target corpus with topic distributions that are significantly different from those in the reference corpus could be considered anomalies.\n",
    "\n",
    "- **Thresholding on Topic Probabilities**: Set thresholds for the probabilities of topics in documents. Documents with topic probabilities that exceed these thresholds (either too high or too low) can be flagged as anomalies.\n",
    "\n",
    "It's important to note that anomaly detection using LDA requires careful interpretation and validation, as the definition of an anomaly can be context-dependent. Additionally, combining LDA with other text analysis and machine learning techniques can improve the effectiveness of anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640b1e72-6794-495d-8dea-ff1119deb00b",
   "metadata": {},
   "source": [
    "## Advanced examples\n",
    "\n",
    "- Using [BERTopic](https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html) and OpenAI ChatGPT for coherent topic labeling and formation, and implemintation [example](https://medium.com/python-in-plain-english/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52666247-0c4e-4422-a55d-8b40f4d5bbc2",
   "metadata": {},
   "source": [
    "# Example 1: Topic Modeling with Gensim\n",
    "\n",
    "Topic Modeling is a technique to extract the hidden topics from large volumes of text.\n",
    "\n",
    "**Latent Dirichlet Allocation(LDA)** is a popular algorithm for topic modeling with excellent implementations in the Python’s Gensim package. \n",
    "\n",
    "The challenge, however, is how to extract **good quality of topics** that are **clear**, **segregated** and **meaningful**. This depends heavily on the quality of text preprocessing and the strategy of finding the optimal number of topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae03cab-90a2-4a3d-8ac5-9f7cc5fb8b94",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "One of the primary applications of natural language processing is to automatically extract what topics people are discussing from large volumes of text. Some examples of large text could be feeds from social media, customer reviews of hotels, movies, etc, user feedbacks, news stories, e-mails of customer complaints etc.\n",
    "\n",
    "Knowing what people are talking about and understanding their problems and opinions is highly valuable to businesses, administrators, political campaigns. And it’s really hard to manually read through such large volumes and compile the topics.\n",
    "\n",
    "Thus is required an automated algorithm that can read through the text documents and automatically output the topics discussed.\n",
    "\n",
    "- This example: We will take a real example of the **20 Newsgroups’ dataset** and use LDA to extract the naturally discussed topics. I will be using the **Latent Dirichlet Allocation (LDA)** from **Gensim** package along with the **Mallet’s implementation** (via Gensim). Mallet has an efficient implementation of the LDA. It is known to run faster and gives better topics segregation. We will also extract the volume and percentage contribution of each topic to get an idea of how important a topic is.\n",
    "\n",
    "### What does LDA do?\n",
    "\n",
    "LDA's approach to topic modeling is it considers each document as a collection of topics in a certain proportion. And each topic as a collection of keywords, again, in a certain proportion.\n",
    "\n",
    "Once you provide the algorithm with the number of topics, all it does it to rearrange the topics distribution within the documents and keywords distribution within the topics to obtain a good composition of topic-keywords distribution.\n",
    "\n",
    "**A topic** is nothing but a collection of dominant keywords that are typical representatives. Just by looking at the keywords, you can identify what the topic is all about.\n",
    "\n",
    "The following are key factors to obtaining good segregation topics:\n",
    "\n",
    "- The quality of text processing.\n",
    "- The variety of topics the text talks about.\n",
    "- The choice of topic modeling algorithm.\n",
    "- The number of topics fed to the algorithm.\n",
    "- The algorithms tuning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd85aa9e-1812-4f78-a0a6-e92888f8c9e8",
   "metadata": {},
   "source": [
    "## Prerequisites – Download and install necessary packages and files and import packages\n",
    "\n",
    "We will need the `stopwords` from **NLTK** and `spacy`’s `en_core_web_sm` model (used to be simply `en` denoting a model pretained on English language) for text pre-processing. Later, we will be using the `spacy` model for lemmatization.\n",
    "\n",
    "The core packages are `re`, `gensim`, `spacy` and `pyLDAvis`. Besides this we will also using `matplotlib`, `numpy` and `pandas` for data handling and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dbefc2e9-3f74-42af-a3ff-92460d421814",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vitali\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.4)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (60.5.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.5)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.62.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vitali\\appdata\\roaming\\python\\python39\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\python39\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;3m[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use\n",
      "the full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2980716-cd7d-4e5d-a5d4-e7fabe339b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vitali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint  # data pretty printer - provides a capability to “pretty-print” arbitrary Python data structures\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import utsnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99da3c-1ee0-4450-9e0b-16a6bcfb259e",
   "metadata": {},
   "source": [
    "## Get data\n",
    "\n",
    "We will be using the 20-Newsgroups dataset for this exercise. This version of the dataset contains about 11k newsgroups posts from 20 different topics. This is available as [newsgroups.json](https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json).\n",
    "\n",
    "This is imported using `pandas.read_json` and the resulting dataset has 3 columns as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3feb1e5-2690-48bb-89f4-a650889ab217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
      " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
      " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
      " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
      " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
      " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\\...</td>\n",
       "      <td>16</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From: bmdelane@quads.uchicago.edu (brian manni...</td>\n",
       "      <td>13</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From: bgrubb@dante.nmsu.edu (GRUBB)\\nSubject: ...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubje...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "5  From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\\...      16   \n",
       "6  From: bmdelane@quads.uchicago.edu (brian manni...      13   \n",
       "7  From: bgrubb@dante.nmsu.edu (GRUBB)\\nSubject: ...       3   \n",
       "8  From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...       2   \n",
       "9  From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubje...       4   \n",
       "\n",
       "               target_names  \n",
       "0                 rec.autos  \n",
       "1     comp.sys.mac.hardware  \n",
       "2     comp.sys.mac.hardware  \n",
       "3             comp.graphics  \n",
       "4                 sci.space  \n",
       "5        talk.politics.guns  \n",
       "6                   sci.med  \n",
       "7  comp.sys.ibm.pc.hardware  \n",
       "8   comp.os.ms-windows.misc  \n",
       "9     comp.sys.mac.hardware  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb1627-2901-4d84-a48a-512d4c028b4e",
   "metadata": {},
   "source": [
    "## Pre-process text\n",
    "\n",
    "- Prepare Stopwords\n",
    "    - We have already downloaded the stopwords. Let’s import them and make it available in `stop_words`.\n",
    "- Remove emails and newline characters\n",
    "    - There are many emails, newline and extra spaces that is quite distracting. Let’s get rid of them using regular expressions.\n",
    "- Tokenize\n",
    "    - Break down each sentence into a list of words through tokenization, while clearing up all the messy text in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e231addd-c528-43b1-85b8-26cb81b2b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "647bdd13-a4ca-4c29-b716-c491e2ed6fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"From: lerxst@wam.umd.edu (where's my thing)\\n\"\n",
      " 'Subject: WHAT car is this!?\\n'\n",
      " 'Nntp-Posting-Host: rac3.wam.umd.edu\\n'\n",
      " 'Organization: University of Maryland, College Park\\n'\n",
      " 'Lines: 15\\n'\n",
      " '\\n'\n",
      " ' I was wondering if anyone out there could enlighten me on this car I saw\\n'\n",
      " 'the other day. It was a 2-door sports car, looked to be from the late 60s/\\n'\n",
      " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
      " 'addition,\\n'\n",
      " 'the front bumper was separate from the rest of the body. This is \\n'\n",
      " 'all I know. If anyone can tellme a model name, engine specs, years\\n'\n",
      " 'of production, where this car is made, history, or whatever info you\\n'\n",
      " 'have on this funky looking car, please e-mail.\\n'\n",
      " '\\n'\n",
      " 'Thanks,\\n'\n",
      " '- IL\\n'\n",
      " '   ---- brought to you by your neighborhood Lerxst ----\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb794f51-1f32-49fd-9112-67983be2288c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: '\n",
      " 'rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: '\n",
      " '15 I was wondering if anyone out there could enlighten me on this car I saw '\n",
      " 'the other day. It was a 2-door sports car, looked to be from the late 60s/ '\n",
      " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
      " 'addition, the front bumper was separate from the rest of the body. This is '\n",
      " 'all I know. If anyone can tellme a model name, engine specs, years of '\n",
      " 'production, where this car is made, history, or whatever info you have on '\n",
      " 'this funky looking car, please e-mail. Thanks, - IL ---- brought to you by '\n",
      " 'your neighborhood Lerxst ---- ']\n"
     ]
    }
   ],
   "source": [
    "# Remove Emails\n",
    "data = [re.sub(r'\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub(r'\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(r\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bbcdb4-acd2-4726-bd09-25a6c501ea8f",
   "metadata": {},
   "source": [
    "Tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.\n",
    "\n",
    "Gensim’s `simple_preprocess()` is great for this. Additionally I have set `deacc=True` to remove the punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85d920fd-4850-4e98-b6a0-451f88172851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words and Clean-up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d657532f-631e-4c6d-8458-58550c0a1bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: a list of strings\n",
      "\u001b[91mFrom: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: 15 I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is all I know. If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail. Thanks, - IL ---- brought to you by your neighborhood Lerxst ---- \u001b[0m\n",
      "\u001b[92mFrom: (Guy Kuo) Subject: SI Clock Poll - Final Call Summary: Final call for SI clock reports Keywords: SI,acceleration,clock,upgrade Article-I.D.: shelley.1qvfo9INNc3s Organization: University of Washington Lines: 11 NNTP-Posting-Host: carson.u.washington.edu A fair number of brave souls who upgraded their SI clock oscillator have shared their experiences for this poll. Please send a brief message detailing your experiences with the procedure. Top speed attained, CPU rated speed, add on cards and adapters, heat sinks, hour of usage per day, floppy disk functionality with 800 and 1.4 m floppies are especially requested. I will be summarizing in the next two days, so please add to the network knowledge base if you have done the clock upgrade and havent answered this poll. Thanks. Guy Kuo \u001b[0m\n",
      "\u001b[94mFrom: (Thomas E Willis) Subject: PB questions... Organization: Purdue University Engineering Computer Network Distribution: usa Lines: 36 well folks, my mac plus finally gave up the ghost this weekend after starting life as a 512k way back in 1985. sooo, im in the market for a new machine a bit sooner than i intended to be... im looking into picking up a powerbook 160 or maybe 180 and have a bunch of questions that (hopefully) somebody can answer: * does anybody know any dirt on when the next round of powerbook introductions are expected? id heard the 185c was supposed to make an appearence \"this summer\" but havent heard anymore on it - and since i dont have access to macleak, i was wondering if anybody out there had more info... * has anybody heard rumors about price drops to the powerbook line like the ones the duos just went through recently? * whats the impression of the display on the 180? i could probably swing a 180 if i got the 80Mb disk rather than the 120, but i dont really have a feel for how much \"better\" the display is (yea, it looks great in the store, but is that all \"wow\" or is it really that good?). could i solicit some opinions of people who use the 160 and 180 day-to-day on if its worth taking the disk size and money hit to get the active display? (i realize this is a real subjective question, but ive only played around with the machines in a computer store breifly and figured the opinions of somebody who actually uses the machine daily might prove helpful). * how well does hellcats perform? ;) thanks a bunch in advance for any info - if you could email, ill post a summary (news reading time is at a premium with finals just around the corner... :( ) -- Tom Willis \\ \\ Purdue Electrical Engineering --------------------------------------------------------------------------- \"Convictions are more dangerous enemies of truth than lies.\" - F. W. Nietzsche \u001b[0m\n",
      "\u001b[93mFrom: (Joe Green) Subject: Re: Weitek P9000 ? Organization: Harris Computer Systems Division Lines: 14 Distribution: world NNTP-Posting-Host: amber.ssd.csd.harris.com X-Newsreader: TIN [version 1.1 PL9] Robert J.C. Kyanko wrote: > writes in article > > Anyone know about the Weitek P9000 graphics chip? > As far as the low-level stuff goes, it looks pretty nice. Its got this > quadrilateral fill command that requires just the four points. Do you have Weiteks address/phone number? Id like to get some information about this chip. -- Joe Green Harris Corporation Computer Systems Division \"The only thing that really scares me is a person with no sense of humor.\" -- Jonathan Winters \u001b[0m\n",
      "\n",
      "\n",
      "Data Type: a list of lists of strings:\n",
      "\u001b[91m[from wheres my thing subject what car is this nntp posting host rac wam umd edu organization university of maryland college park lines was wondering if anyone out there could enlighten me on this car saw the other day it was door sports car looked to be from the late early it was called bricklin the doors were really small in addition the front bumper was separate from the rest of the body this is all know if anyone can tellme model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please mail thanks il brought to you by your neighborhood lerxst ]\u001b[0m\n",
      "\u001b[92m[from guy kuo subject si clock poll final call summary final call for si clock reports keywords si acceleration clock upgrade article shelley qvfo innc organization university of washington lines nntp posting host carson washington edu fair number of brave souls who upgraded their si clock oscillator have shared their experiences for this poll please send brief message detailing your experiences with the procedure top speed attained cpu rated speed add on cards and adapters heat sinks hour of usage per day floppy disk functionality with and floppies are especially requested will be summarizing in the next two days so please add to the network knowledge base if you have done the clock upgrade and havent answered this poll thanks guy kuo ]\u001b[0m\n",
      "\u001b[94m[from thomas willis subject pb questions organization purdue university engineering computer network distribution usa lines well folks my mac plus finally gave up the ghost this weekend after starting life as way back in sooo im in the market for new machine bit sooner than intended to be im looking into picking up powerbook or maybe and have bunch of questions that hopefully somebody can answer does anybody know any dirt on when the next round of powerbook introductions are expected id heard the was supposed to make an appearence this summer but havent heard anymore on it and since dont have access to macleak was wondering if anybody out there had more info has anybody heard rumors about price drops to the powerbook line like the ones the duos just went through recently whats the impression of the display on the could probably swing if got the mb disk rather than the but dont really have feel for how much better the display is yea it looks great in the store but is that all wow or is it really that good could solicit some opinions of people who use the and day to day on if its worth taking the disk size and money hit to get the active display realize this is real subjective question but ive only played around with the machines in computer store breifly and figured the opinions of somebody who actually uses the machine daily might prove helpful how well does hellcats perform thanks bunch in advance for any info if you could email ill post summary news reading time is at premium with finals just around the corner tom willis purdue electrical engineering convictions are more dangerous enemies of truth than lies nietzsche ]\u001b[0m\n",
      "\u001b[93m[from joe green subject re weitek organization harris computer systems division lines distribution world nntp posting host amber ssd csd harris com newsreader tin version pl robert kyanko wrote writes in article anyone know about the weitek graphics chip as far as the low level stuff goes it looks pretty nice its got this quadrilateral fill command that requires just the four points do you have weiteks address phone number id like to get some information about this chip joe green harris corporation computer systems division the only thing that really scares me is person with no sense of humor jonathan winters ]\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utsnlp.print_colored_text(data)\n",
    "utsnlp.print_colored_text(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3730b508-aca7-4c49-a4c0-184bc2bcd15a",
   "metadata": {},
   "source": [
    "## Creating Bigram (or Trigram) Models\n",
    "\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n",
    "\n",
    "Some examples in our example are: ‘front_bumper’, ‘oil_leak’, ‘maryland_college_park’ etc.\n",
    "\n",
    "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da56c033-26d9-4b44-a023-07bce3012dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp_posting_host', 'rac_wam_umd_edu', 'organization', 'university', 'of', 'maryland_college_park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front_bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "300f654e-f174-4749-92d6-66a9cddbc4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the models to the data\n",
    "bigram_data = [bigram_mod[doc] for doc in data_words]\n",
    "trigram_data = [trigram_mod[bigram_mod[doc]] for doc in data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24daabd8-f35e-47bb-ad64-4dd416649a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "from wheres my thing subject what car is this nntp posting host rac wam umd edu organization university of maryland college park lines was wondering if anyone out there could enlighten me on this car saw the other day it was door sports car looked to be from the late early it was called bricklin the doors were really small in addition the front bumper was separate from the rest of the body this is all know if anyone can tellme model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please mail thanks il brought to you by your neighborhood lerxst\n",
      "from guy kuo subject si clock poll final call summary final call for si clock reports keywords si acceleration clock upgrade article shelley qvfo innc organization university of washington lines nntp posting host carson washington edu fair number of brave souls who upgraded their si clock oscillator have shared their experiences for this poll please send brief message detailing your experiences with the procedure top speed attained cpu rated speed add on cards and adapters heat sinks hour of usage per day floppy disk functionality with and floppies are especially requested will be summarizing in the next two days so please add to the network knowledge base if you have done the clock upgrade and havent answered this poll thanks guy kuo\n",
      "from thomas willis subject pb questions organization purdue university engineering computer network distribution usa lines well folks my mac plus finally gave up the ghost this weekend after starting life as way back in sooo im in the market for new machine bit sooner than intended to be im looking into picking up powerbook or maybe and have bunch of questions that hopefully somebody can answer does anybody know any dirt on when the next round of powerbook introductions are expected id heard the was supposed to make an appearence this summer but havent heard anymore on it and since dont have access to macleak was wondering if anybody out there had more info has anybody heard rumors about price drops to the powerbook line like the ones the duos just went through recently whats the impression of the display on the could probably swing if got the mb disk rather than the but dont really have feel for how much better the display is yea it looks great in the store but is that all wow or is it really that good could solicit some opinions of people who use the and day to day on if its worth taking the disk size and money hit to get the active display realize this is real subjective question but ive only played around with the machines in computer store breifly and figured the opinions of somebody who actually uses the machine daily might prove helpful how well does hellcats perform thanks bunch in advance for any info if you could email ill post summary news reading time is at premium with finals just around the corner tom willis purdue electrical engineering convictions are more dangerous enemies of truth than lies nietzsche\n",
      "\n",
      "Bigram:\n",
      "from wheres my thing subject what car is this \u001b[91mnntp_posting\u001b[0m host \u001b[91mrac_wam\u001b[0m \u001b[91mumd_edu\u001b[0m organization university of \u001b[91mmaryland_college\u001b[0m park lines was wondering if anyone out there could enlighten me on this car saw the other day it was door sports car looked to be from the late early it was called bricklin the doors were really small in addition the \u001b[91mfront_bumper\u001b[0m was separate from the rest of the body this is all know if anyone can tellme model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please mail thanks il brought to you by your neighborhood lerxst\n",
      "from \u001b[91mguy_kuo\u001b[0m subject si clock poll final call summary final call for si clock reports keywords si acceleration clock upgrade article shelley qvfo innc organization university of washington lines \u001b[91mnntp_posting\u001b[0m host \u001b[91mcarson_washington\u001b[0m edu fair number of brave souls who upgraded their si clock oscillator have shared their experiences for this poll please send brief message detailing your experiences with the procedure top speed attained cpu rated speed add on cards and adapters \u001b[91mheat_sinks\u001b[0m hour of usage per day \u001b[91mfloppy_disk\u001b[0m functionality with and floppies are especially requested will be summarizing in the next two days so please add to the network knowledge base if you have done the clock upgrade and havent answered this poll thanks \u001b[91mguy_kuo\u001b[0m\n",
      "from thomas willis subject pb questions organization \u001b[91mpurdue_university\u001b[0m engineering computer network \u001b[91mdistribution_usa\u001b[0m lines well folks my mac plus finally gave up the ghost this weekend after starting life as way back in sooo im in the market for new machine bit sooner than intended to be im looking into picking up powerbook or maybe and have bunch of questions that hopefully somebody can answer does anybody know any dirt on when the next round of powerbook introductions are expected id heard the was supposed to make an appearence this summer but havent heard anymore on it and since dont have access to macleak was wondering if anybody out there had more info has anybody heard rumors about price drops to the powerbook line like the ones the duos just went through recently whats the impression of the display on the could probably swing if got the mb disk rather than the but dont really have feel for how much better the display is yea it looks great in the store but is that all wow or is it really that good could solicit some opinions of people who use the and day to day on if its worth taking the disk size and money hit to get the active display realize this is real subjective question but ive only played around with the machines in computer store breifly and figured the opinions of somebody who actually uses the machine daily might prove helpful how well does hellcats perform thanks bunch in advance for any info if you could email ill post summary news reading time is at premium with finals just around the corner tom willis purdue \u001b[91melectrical_engineering\u001b[0m convictions are more \u001b[91mdangerous_enemies\u001b[0m of truth than lies nietzsche\n",
      "\n",
      "Trigram:\n",
      "from wheres my thing subject what car is this \u001b[91mnntp_posting_host\u001b[0m \u001b[91mrac_wam_umd_edu\u001b[0m organization university of \u001b[91mmaryland_college_park\u001b[0m lines was wondering if anyone out there could enlighten me on this car saw the other day it was door sports car looked to be from the late early it was called bricklin the doors were really small in addition the \u001b[91mfront_bumper\u001b[0m was separate from the rest of the body this is all know if anyone can tellme model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please mail thanks il brought to you by your neighborhood lerxst\n",
      "from \u001b[91mguy_kuo\u001b[0m subject si clock poll final call summary final call for si clock reports keywords si acceleration clock upgrade \u001b[91marticle_shelley\u001b[0m qvfo innc organization university of washington \u001b[91mlines_nntp_posting\u001b[0m host \u001b[91mcarson_washington_edu\u001b[0m fair number of brave souls who upgraded their si clock oscillator have shared their experiences for this poll please send brief message detailing your experiences with the procedure top speed attained cpu rated speed add on cards and adapters \u001b[91mheat_sinks\u001b[0m hour of usage per day \u001b[91mfloppy_disk\u001b[0m functionality with and floppies are especially requested will be summarizing in the next two days so please add to the network knowledge base if you have done the clock upgrade and havent answered this poll thanks \u001b[91mguy_kuo\u001b[0m\n",
      "from thomas willis subject pb questions organization \u001b[91mpurdue_university_engineering\u001b[0m computer network \u001b[91mdistribution_usa\u001b[0m lines well folks my mac plus finally gave up the ghost this weekend after starting life as way back in sooo im in the market for new machine bit sooner than intended to be im looking into picking up powerbook or maybe and have bunch of questions that hopefully somebody can answer does anybody know any dirt on when the next round of powerbook introductions are expected id heard the was supposed to make an appearence this summer but havent heard anymore on it and since dont have access to macleak was wondering if anybody out there had more info has anybody heard rumors about price drops to the powerbook line like the ones the duos just went through recently whats the impression of the display on the could probably swing if got the mb disk rather than the but dont really have feel for how much better the display is yea it looks great in the store but is that all wow or is it really that good could solicit some opinions of people who use the and day to day on if its worth taking the disk size and money hit to get the active display realize this is real subjective question but ive only played around with the machines in computer store breifly and figured the opinions of somebody who actually uses the machine daily might prove helpful how well does hellcats perform thanks bunch in advance for any info if you could email ill post summary news reading time is at premium with finals just around the corner tom willis purdue \u001b[91melectrical_engineering\u001b[0m convictions are more \u001b[91mdangerous_enemies\u001b[0m of truth than lies nietzsche\n"
     ]
    }
   ],
   "source": [
    "nDocs = 3\n",
    "red_color = '\\033[91m'  # ANSI escape code for red color\n",
    "reset_color = '\\033[0m'  # ANSI escape code to reset color\n",
    "\n",
    "# Function to color words with underscores in red\n",
    "def color_words_with_underscores(words):\n",
    "    colored_words = []\n",
    "    for word in words:\n",
    "        if '_' in word:\n",
    "            colored_words.append(f\"{red_color}{word}{reset_color}\")\n",
    "        else:\n",
    "            colored_words.append(word)\n",
    "    return \" \".join(colored_words)\n",
    "\n",
    "# Print the original, bigram, and trigram data for comparison\n",
    "print(\"Original:\")\n",
    "for doc in data_words[:nDocs]:\n",
    "    print(color_words_with_underscores(doc))\n",
    "\n",
    "print(\"\\nBigram:\")\n",
    "for doc in bigram_data[:nDocs]:\n",
    "    print(color_words_with_underscores(doc))\n",
    "\n",
    "print(\"\\nTrigram:\")\n",
    "for doc in trigram_data[:nDocs]:\n",
    "    print(color_words_with_underscores(doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f4940-f181-4787-80d6-d904c163a9f4",
   "metadata": {},
   "source": [
    "The bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d8d9b92-ffab-42dd-a9f5-3477d7b3bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a5de94c-3be0-4cb7-8fbc-31693bc53567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['s', 'thing', 'car', 'nntp_poste', 'host', 'rac_wam', 'university', 'park', 'line', 'wonder', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'door', 'really', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45a713-abe4-46fa-a28f-a5c3339c6529",
   "metadata": {},
   "source": [
    "## Create the Dictionary and Corpus needed for Topic Modeling\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary(`id2word`) and the `corpus`. \n",
    "\n",
    "Let’s create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d76020bd-e359-498b-9d4b-5e4622602e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 5), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec34d67d-8214-473a-814e-2119a9321bd9",
   "metadata": {},
   "source": [
    "Gensim creates a unique `id` for each word in the document. The produced corpus shown above is a mapping of `(word_id, word_frequency)`.\n",
    "\n",
    "For example, `(0, 1)` above implies, word `id 0` occurs once in the first document. Likewise, word `id 4` occurs 5 times and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "032fc6af-b956-47b4-a632-9ccdddbe4653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('addition', 1),\n",
       "  ('body', 1),\n",
       "  ('bring', 1),\n",
       "  ('call', 1),\n",
       "  ('car', 5),\n",
       "  ('day', 1),\n",
       "  ('door', 2),\n",
       "  ('early', 1),\n",
       "  ('engine', 1),\n",
       "  ('enlighten', 1),\n",
       "  ('funky', 1),\n",
       "  ('history', 1),\n",
       "  ('host', 1),\n",
       "  ('info', 1),\n",
       "  ('know', 1),\n",
       "  ('late', 1),\n",
       "  ('lerxst', 1),\n",
       "  ('line', 1),\n",
       "  ('look', 2),\n",
       "  ('mail', 1),\n",
       "  ('make', 1),\n",
       "  ('model', 1),\n",
       "  ('name', 1),\n",
       "  ('neighborhood', 1),\n",
       "  ('nntp_poste', 1),\n",
       "  ('park', 1),\n",
       "  ('production', 1),\n",
       "  ('rac_wam', 1),\n",
       "  ('really', 1),\n",
       "  ('rest', 1),\n",
       "  ('s', 1),\n",
       "  ('see', 1),\n",
       "  ('separate', 1),\n",
       "  ('small', 1),\n",
       "  ('spec', 1),\n",
       "  ('sport', 1),\n",
       "  ('thank', 1),\n",
       "  ('thing', 1),\n",
       "  ('university', 1),\n",
       "  ('wonder', 1),\n",
       "  ('year', 1)]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109484a-f71e-4ed5-8214-2d8800ed7dda",
   "metadata": {},
   "source": [
    "## Building the Topic Model\n",
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, `alpha` and `eta` are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to *1.0/num_topics* prior.\n",
    "\n",
    "`chunksize` is the number of documents to be used in each training chunk. `update_every` determines how often the model parameters should be updated and `passes` is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e68eef0c-1ae5-4b72-90ca-f75edd0fd9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b56e2e-f663-42ee-ad1b-fc44efd683c3",
   "metadata": {},
   "source": [
    "## View the topics in LDA model\n",
    "The above LDA model is built with 20 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of each keyword using `lda_model.print_topics()` as shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff3889ce-9eb6-411a-a36e-06dfb15bf58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.021*\"research\" + 0.019*\"information\" + 0.019*\"high\" + 0.019*\"report\" + '\n",
      "  '0.018*\"player\" + 0.016*\"service\" + 0.015*\"rate\" + 0.014*\"design\" + '\n",
      "  '0.013*\"season\" + 0.012*\"low\"'),\n",
      " (1,\n",
      "  '0.077*\"team\" + 0.072*\"game\" + 0.053*\"play\" + 0.050*\"faith\" + 0.049*\"win\" + '\n",
      "  '0.031*\"belief\" + 0.025*\"atheist\" + 0.025*\"year\" + 0.018*\"wing\" + '\n",
      "  '0.018*\"score\"'),\n",
      " (2,\n",
      "  '0.106*\"space\" + 0.029*\"notice\" + 0.029*\"launch\" + 0.026*\"earth\" + '\n",
      "  '0.024*\"mission\" + 0.024*\"orbit\" + 0.023*\"external\" + 0.020*\"vehicle\" + '\n",
      "  '0.019*\"satellite\" + 0.019*\"door\"'),\n",
      " (3,\n",
      "  '0.022*\"say\" + 0.019*\"people\" + 0.017*\"reason\" + 0.017*\"believe\" + '\n",
      "  '0.015*\"evidence\" + 0.014*\"mean\" + 0.012*\"point\" + 0.012*\"question\" + '\n",
      "  '0.011*\"many\" + 0.010*\"claim\"'),\n",
      " (4,\n",
      "  '0.078*\"book\" + 0.044*\"science\" + 0.042*\"reference\" + 0.036*\"pin\" + '\n",
      "  '0.032*\"section\" + 0.025*\"faq\" + 0.024*\"author\" + 0.023*\"copy\" + '\n",
      "  '0.023*\"reality\" + 0.022*\"internal\"'),\n",
      " (5,\n",
      "  '0.065*\"cost\" + 0.059*\"model\" + 0.039*\"character\" + 0.036*\"picture\" + '\n",
      "  '0.036*\"format\" + 0.032*\"quality\" + 0.032*\"associate\" + 0.028*\"handle\" + '\n",
      "  '0.023*\"hole\" + 0.023*\"gift\"'),\n",
      " (6,\n",
      "  '0.032*\"system\" + 0.028*\"use\" + 0.024*\"program\" + 0.023*\"file\" + '\n",
      "  '0.018*\"card\" + 0.016*\"run\" + 0.014*\"software\" + 0.014*\"bit\" + '\n",
      "  '0.013*\"machine\" + 0.013*\"problem\"'),\n",
      " (7,\n",
      "  '0.092*\"moral\" + 0.056*\"property\" + 0.045*\"serial\" + 0.036*\"lock\" + '\n",
      "  '0.022*\"positively\" + 0.021*\"intent\" + 0.018*\"alarm\" + 0.012*\"converter\" + '\n",
      "  '0.011*\"unnecessary\" + 0.007*\"provision\"'),\n",
      " (8,\n",
      "  '0.249*\"window\" + 0.057*\"monitor\" + 0.055*\"normal\" + 0.041*\"do\" + '\n",
      "  '0.032*\"font\" + 0.023*\"left\" + 0.020*\"widget\" + 0.019*\"please_respond\" + '\n",
      "  '0.017*\"environment\" + 0.017*\"trivial\"'),\n",
      " (9,\n",
      "  '0.061*\"child\" + 0.028*\"church\" + 0.027*\"woman\" + 0.025*\"armenian\" + '\n",
      "  '0.022*\"authority\" + 0.020*\"community\" + 0.019*\"greek\" + 0.017*\"period\" + '\n",
      "  '0.017*\"turk\" + 0.016*\"soldier\"'),\n",
      " (10,\n",
      "  '0.765*\"ax\" + 0.035*\"physical\" + 0.024*\"graphic\" + 0.014*\"direct\" + '\n",
      "  '0.011*\"convert\" + 0.006*\"daughter\" + 0.006*\"capture\" + 0.005*\"human_being\" '\n",
      "  '+ 0.004*\"split\" + 0.003*\"accomplish\"'),\n",
      " (11,\n",
      "  '0.130*\"line\" + 0.076*\"organization\" + 0.074*\"write\" + 0.063*\"article\" + '\n",
      "  '0.056*\"nntp_poste\" + 0.050*\"host\" + 0.029*\"reply\" + 0.024*\"thank\" + '\n",
      "  '0.018*\"university\" + 0.013*\"post\"'),\n",
      " (12,\n",
      "  '0.072*\"plane\" + 0.030*\"hi\" + 0.021*\"subscription\" + 0.020*\"steve\" + '\n",
      "  '0.015*\"divide\" + 0.011*\"evolve\" + 0.010*\"intersection\" + 0.010*\"rip\" + '\n",
      "  '0.008*\"upcoming\" + 0.007*\"script\"'),\n",
      " (13,\n",
      "  '0.031*\"people\" + 0.028*\"state\" + 0.018*\"gun\" + 0.017*\"government\" + '\n",
      "  '0.017*\"law\" + 0.016*\"right\" + 0.015*\"kill\" + 0.013*\"death\" + 0.011*\"live\" + '\n",
      "  '0.011*\"force\"'),\n",
      " (14,\n",
      "  '0.141*\"drug\" + 0.029*\"film\" + 0.026*\"movie\" + 0.025*\"stereo\" + '\n",
      "  '0.024*\"japanese\" + 0.022*\"deficit\" + 0.020*\"plot\" + 0.014*\"mad\" + '\n",
      "  '0.009*\"harley\" + 0.007*\"deck\"'),\n",
      " (15,\n",
      "  '0.061*\"box\" + 0.050*\"club\" + 0.041*\"modem\" + 0.041*\"status\" + '\n",
      "  '0.030*\"primary\" + 0.029*\"routine\" + 0.029*\"spec\" + 0.026*\"sufficient\" + '\n",
      "  '0.023*\"public_access\" + 0.023*\"automatically\"'),\n",
      " (16,\n",
      "  '0.152*\"drive\" + 0.091*\"car\" + 0.036*\"bike\" + 0.024*\"engine\" + 0.023*\"nhl\" + '\n",
      "  '0.022*\"ride\" + 0.018*\"road\" + 0.017*\"weight\" + 0.016*\"mile\" + '\n",
      "  '0.015*\"ground\"'),\n",
      " (17,\n",
      "  '0.113*\"patient\" + 0.060*\"disease\" + 0.054*\"scientific\" + '\n",
      "  '0.050*\"computer_science\" + 0.043*\"animal\" + 0.041*\"health\" + '\n",
      "  '0.040*\"treatment\" + 0.037*\"medical\" + 0.033*\"dog\" + 0.030*\"study\"'),\n",
      " (18,\n",
      "  '0.023*\"get\" + 0.018*\"go\" + 0.015*\"good\" + 0.015*\"time\" + 0.015*\"know\" + '\n",
      "  '0.014*\"make\" + 0.013*\"well\" + 0.013*\"think\" + 0.012*\"see\" + 0.010*\"take\"'),\n",
      " (19,\n",
      "  '0.106*\"key\" + 0.043*\"test\" + 0.032*\"public\" + 0.031*\"encryption\" + '\n",
      "  '0.028*\"security\" + 0.028*\"server\" + 0.022*\"clipper\" + 0.021*\"chip\" + '\n",
      "  '0.018*\"secure\" + 0.018*\"message\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0facd4c0-f0f2-4f47-8186-83a8f9a42f25",
   "metadata": {},
   "source": [
    "### How to interpret this?\n",
    "\n",
    "Topic 0, for example, is represented below. It means that the top keywords that contribute to this topic are as below and the weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df491fc3-875e-486c-811f-a2924d5a9b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,\n",
      " '0.021*\"research\" + 0.019*\"information\" + 0.019*\"high\" + 0.019*\"report\" + '\n",
      " '0.018*\"player\" + 0.016*\"service\" + 0.015*\"rate\" + 0.014*\"design\" + '\n",
      " '0.013*\"season\" + 0.012*\"low\"')\n"
     ]
    }
   ],
   "source": [
    "topicID=0\n",
    "pprint(lda_model.print_topics(num_words=10)[topicID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "99056f73-efc1-488d-a48c-a13f5a1a5ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of topics from the LDA model\n",
    "num_topics = lda_model.num_topics\n",
    "num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b292955-ecdc-48fd-82f4-116063625cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['research & information & high',\n",
       " 'team & game & play',\n",
       " 'space & notice & launch',\n",
       " 'say & people & reason',\n",
       " 'book & science & reference',\n",
       " 'cost & model & character',\n",
       " 'system & use & program',\n",
       " 'moral & property & serial',\n",
       " 'window & monitor & normal',\n",
       " 'child & church & woman',\n",
       " 'ax & physical & graphic',\n",
       " 'line & organization & write',\n",
       " 'plane & hi & subscription',\n",
       " 'people & state & gun',\n",
       " 'drug & film & movie',\n",
       " 'box & club & modem',\n",
       " 'drive & car & bike',\n",
       " 'patient & disease & scientific',\n",
       " 'get & go & good',\n",
       " 'key & test & public']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_topic_labels(lda_model, num_topics, topn=2):\n",
    "    topic_labels = []\n",
    "    for i in range(num_topics):\n",
    "        # Get the top 2 words for each topic\n",
    "        top_words = lda_model.show_topic(i, topn=topn)\n",
    "        # Combine the top words to form a label\n",
    "        label = ' & '.join([word for word, _ in top_words])\n",
    "        topic_labels.append(label)\n",
    "    return topic_labels\n",
    "\n",
    "# Get labels for all topics\n",
    "labels = get_topic_labels(lda_model, num_topics=num_topics, topn=3)\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "af60cd32-72f9-4e9c-b511-43c5cd464ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 (\u001b[91mresearch & information & high\u001b[0m): 0.021*\"research\" + 0.019*\"information\" + 0.019*\"high\" + 0.019*\"report\" + 0.018*\"player\" + 0.016*\"service\" + 0.015*\"rate\" + 0.014*\"design\" + 0.013*\"season\" + 0.012*\"low\"\n",
      "Topic 1 (\u001b[91mteam & game & play\u001b[0m): 0.077*\"team\" + 0.072*\"game\" + 0.053*\"play\" + 0.050*\"faith\" + 0.049*\"win\" + 0.031*\"belief\" + 0.025*\"atheist\" + 0.025*\"year\" + 0.018*\"wing\" + 0.018*\"score\"\n",
      "Topic 2 (\u001b[91mspace & notice & launch\u001b[0m): 0.106*\"space\" + 0.029*\"notice\" + 0.029*\"launch\" + 0.026*\"earth\" + 0.024*\"mission\" + 0.024*\"orbit\" + 0.023*\"external\" + 0.020*\"vehicle\" + 0.019*\"satellite\" + 0.019*\"door\"\n",
      "Topic 3 (\u001b[91msay & people & reason\u001b[0m): 0.022*\"say\" + 0.019*\"people\" + 0.017*\"reason\" + 0.017*\"believe\" + 0.015*\"evidence\" + 0.014*\"mean\" + 0.012*\"point\" + 0.012*\"question\" + 0.011*\"many\" + 0.010*\"claim\"\n",
      "Topic 4 (\u001b[91mbook & science & reference\u001b[0m): 0.078*\"book\" + 0.044*\"science\" + 0.042*\"reference\" + 0.036*\"pin\" + 0.032*\"section\" + 0.025*\"faq\" + 0.024*\"author\" + 0.023*\"copy\" + 0.023*\"reality\" + 0.022*\"internal\"\n",
      "Topic 5 (\u001b[91mcost & model & character\u001b[0m): 0.065*\"cost\" + 0.059*\"model\" + 0.039*\"character\" + 0.036*\"picture\" + 0.036*\"format\" + 0.032*\"quality\" + 0.032*\"associate\" + 0.028*\"handle\" + 0.023*\"hole\" + 0.023*\"gift\"\n",
      "Topic 6 (\u001b[91msystem & use & program\u001b[0m): 0.032*\"system\" + 0.028*\"use\" + 0.024*\"program\" + 0.023*\"file\" + 0.018*\"card\" + 0.016*\"run\" + 0.014*\"software\" + 0.014*\"bit\" + 0.013*\"machine\" + 0.013*\"problem\"\n",
      "Topic 7 (\u001b[91mmoral & property & serial\u001b[0m): 0.092*\"moral\" + 0.056*\"property\" + 0.045*\"serial\" + 0.036*\"lock\" + 0.022*\"positively\" + 0.021*\"intent\" + 0.018*\"alarm\" + 0.012*\"converter\" + 0.011*\"unnecessary\" + 0.007*\"provision\"\n",
      "Topic 8 (\u001b[91mwindow & monitor & normal\u001b[0m): 0.249*\"window\" + 0.057*\"monitor\" + 0.055*\"normal\" + 0.041*\"do\" + 0.032*\"font\" + 0.023*\"left\" + 0.020*\"widget\" + 0.019*\"please_respond\" + 0.017*\"environment\" + 0.017*\"trivial\"\n",
      "Topic 9 (\u001b[91mchild & church & woman\u001b[0m): 0.061*\"child\" + 0.028*\"church\" + 0.027*\"woman\" + 0.025*\"armenian\" + 0.022*\"authority\" + 0.020*\"community\" + 0.019*\"greek\" + 0.017*\"period\" + 0.017*\"turk\" + 0.016*\"soldier\"\n",
      "Topic 10 (\u001b[91max & physical & graphic\u001b[0m): 0.765*\"ax\" + 0.035*\"physical\" + 0.024*\"graphic\" + 0.014*\"direct\" + 0.011*\"convert\" + 0.006*\"daughter\" + 0.006*\"capture\" + 0.005*\"human_being\" + 0.004*\"split\" + 0.003*\"accomplish\"\n",
      "Topic 11 (\u001b[91mline & organization & write\u001b[0m): 0.130*\"line\" + 0.076*\"organization\" + 0.074*\"write\" + 0.063*\"article\" + 0.056*\"nntp_poste\" + 0.050*\"host\" + 0.029*\"reply\" + 0.024*\"thank\" + 0.018*\"university\" + 0.013*\"post\"\n",
      "Topic 12 (\u001b[91mplane & hi & subscription\u001b[0m): 0.072*\"plane\" + 0.030*\"hi\" + 0.021*\"subscription\" + 0.020*\"steve\" + 0.015*\"divide\" + 0.011*\"evolve\" + 0.010*\"intersection\" + 0.010*\"rip\" + 0.008*\"upcoming\" + 0.007*\"script\"\n",
      "Topic 13 (\u001b[91mpeople & state & gun\u001b[0m): 0.031*\"people\" + 0.028*\"state\" + 0.018*\"gun\" + 0.017*\"government\" + 0.017*\"law\" + 0.016*\"right\" + 0.015*\"kill\" + 0.013*\"death\" + 0.011*\"live\" + 0.011*\"force\"\n",
      "Topic 14 (\u001b[91mdrug & film & movie\u001b[0m): 0.141*\"drug\" + 0.029*\"film\" + 0.026*\"movie\" + 0.025*\"stereo\" + 0.024*\"japanese\" + 0.022*\"deficit\" + 0.020*\"plot\" + 0.014*\"mad\" + 0.009*\"harley\" + 0.007*\"deck\"\n",
      "Topic 15 (\u001b[91mbox & club & modem\u001b[0m): 0.061*\"box\" + 0.050*\"club\" + 0.041*\"modem\" + 0.041*\"status\" + 0.030*\"primary\" + 0.029*\"routine\" + 0.029*\"spec\" + 0.026*\"sufficient\" + 0.023*\"public_access\" + 0.023*\"automatically\"\n",
      "Topic 16 (\u001b[91mdrive & car & bike\u001b[0m): 0.152*\"drive\" + 0.091*\"car\" + 0.036*\"bike\" + 0.024*\"engine\" + 0.023*\"nhl\" + 0.022*\"ride\" + 0.018*\"road\" + 0.017*\"weight\" + 0.016*\"mile\" + 0.015*\"ground\"\n",
      "Topic 17 (\u001b[91mpatient & disease & scientific\u001b[0m): 0.113*\"patient\" + 0.060*\"disease\" + 0.054*\"scientific\" + 0.050*\"computer_science\" + 0.043*\"animal\" + 0.041*\"health\" + 0.040*\"treatment\" + 0.037*\"medical\" + 0.033*\"dog\" + 0.030*\"study\"\n",
      "Topic 18 (\u001b[91mget & go & good\u001b[0m): 0.023*\"get\" + 0.018*\"go\" + 0.015*\"good\" + 0.015*\"time\" + 0.015*\"know\" + 0.014*\"make\" + 0.013*\"well\" + 0.013*\"think\" + 0.012*\"see\" + 0.010*\"take\"\n",
      "Topic 19 (\u001b[91mkey & test & public\u001b[0m): 0.106*\"key\" + 0.043*\"test\" + 0.032*\"public\" + 0.031*\"encryption\" + 0.028*\"security\" + 0.028*\"server\" + 0.022*\"clipper\" + 0.021*\"chip\" + 0.018*\"secure\" + 0.018*\"message\"\n"
     ]
    }
   ],
   "source": [
    "# Print all topics with their labels\n",
    "red_color = '\\033[91m'  # ANSI escape code for red color\n",
    "reset_color = '\\033[0m'  # ANSI escape code to reset color\n",
    "\n",
    "for i in range(num_topics):\n",
    "    topic = lda_model.print_topics(num_words=10)[i][1]\n",
    "    print(f\"Topic {i} ({red_color}{labels[i]}{reset_color}):\", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09edf3-9418-40d3-a2f4-4d2e917ceeca",
   "metadata": {},
   "source": [
    "## Compute Model Perplexity and Coherence Score\n",
    "\n",
    "Model perplexity and [topic coherence](https://rare-technologies.com/what-is-topic-coherence/) provide a convenient measure to judge how good a given topic model is. In my experience, topic coherence score, in particular, has been more helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26e0d9d5-52f4-408d-9742-8f8955169629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -13.32460930278773\n",
      "\n",
      "Coherence Score:  0.483541481988623\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a560e-cb77-401b-98b3-137e7b028c3d",
   "metadata": {},
   "source": [
    "## Visualize the topics-keywords\n",
    "\n",
    "Now that the LDA model is built, the next step is to examine the produced topics and the associated keywords. There is no better tool than pyLDAvis package’s interactive chart and is designed to work well with jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2b1aa-af78-4639-ab8c-760c072e92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.show(vis, local=False) # or you can simply run 'vis' for in-notebook view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1e018-aa28-4a29-86c5-5f459edce840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to an HTML file\n",
    "pyLDAvis.save_html(vis, 'lda_topic_visualization.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68f89d-0e65-4f48-be7d-99182fa1b753",
   "metadata": {},
   "source": [
    "### Interpret pyLDAvis’s output\n",
    "\n",
    "Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "A good topic model will have fairly big, **non-overlapping bubbles** scattered throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "Alright, if you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b57f17-ff31-43b4-ad90-04a168352b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
